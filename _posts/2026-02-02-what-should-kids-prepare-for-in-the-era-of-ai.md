---
title: "What Should Kids Prepare For in the Era of AI?"
date: 2026-02-02
tags: [ai, parenting, careers, education, workforce, uncertainty]
header:
  image: /assets/images/headers/kids-ai-career-header.png
---

The skills that make you resilient to AI disruption are exactly the skills that AI makes easy to skip.

That's the paradox I keep circling. To use AI well, you need judgment, taste, and deep expertise. But AI makes it trivially easy to skip the struggle that builds those things. The tool that could make you better makes it harder to become good in the first place.

My kids haven't asked me what they should study yet. When they do, I want to have something better than "study whatever makes you happy." That's what I'd say now, and it's a confession of uncertainty, not wisdom.

Here's what I do know: that uncertainty isn't distributed evenly.

## The Burden Falls on the Wrong People

Mid-career professionals have something hard to automate: tacit knowledge, judgment, context. A senior engineer doesn't just know how to code. She knows which problems are worth solving, how decisions actually get made, why certain elegant approaches fail in practice. This judgment layer sits on years of experience. It's exactly what current AI lacks.

Even under aggressive AGI timelines, established professionals will experience transition, not shock. [Adoption lag](/2026/01/19/the-unbundling-of-work/) protects them. AI capability doesn't equal instant organizational change. By the time AI reshapes their industry, they'll have adapted, pivoted, or retired.

The real uncertainty burden falls on people entering the workforce. And by extension, on the kids we're trying to prepare.

## AI Is a Multiplier

Why does experience matter so much? Here's the mental model I keep coming back to: AI amplifies whatever is already there.

Think of it like a power saw. It makes a master carpenter faster. It makes a novice dangerous. The tool doesn't care which one you are.

If you're smart, disciplined, and curious, AI makes you more so. You move faster. You tackle bigger problems. If you're sloppy and weak at reasoning, AI amplifies that too. You produce more garbage, faster, with higher confidence.

Current AI still hallucinates. It sounds confident when it's wrong. It produces plausible nonsense that requires expertise to catch. Someone has to know what "good" looks like. That someone is usually the person who could have done the work themselves, just slower.

This is why experienced people win with AI, at least for now. They can guide the tool, catch its mistakes, and know when to override it. They have something worth multiplying.

But what if you don't have anything worth multiplying yet?

## The Catch-22 for New Entrants

People entering the workforce face a brutal paradox.

Everyone around them is using AI. They must use AI to keep pace. But they don't yet have deep skills, taste, or judgment. They don't know what good work looks like.

If they rely heavily on AI, they skip the struggle that builds fundamentals. If they avoid AI, they fall behind everyone who didn't.

The paradox, stated plainly: they don't yet have anything for AI to multiply.

## I've Seen This at Work

This isn't theoretical. When engineers are reward-hacking for ticket completion, you can tell. In code review, everything looks fine. Elegant, even. But then you run it. Things break. Images render wrong. Edge cases explode. The code passed review because it *looked* right, but no one verified it actually *worked* right. Trust but verify became trust and ship.

The downstream effect is worse than the bad code. Senior engineers spend their time reviewing and fixing instead of building. This makes them skeptical of AI. They've seen what happens when people who don't understand the work use tools that don't understand it either.

The spiral: juniors misuse AI, seniors clean up, seniors become skeptical, adoption slows, and the juniors still aren't building fundamentals. Everyone loses.

## The Calculator Came After the Multiplication Tables

This tension isn't entirely new. Schools faced a version of it with calculators in the 1970s and 80s.

The consensus took decades: fundamentals first. Students learn arithmetic (K-5), then get calculators (grades 6-7). Research confirmed the intuition. Premature use creates dependency without understanding. Appropriate use, after fundamentals are established, doesn't harm development and often helps.

The workforce has no equivalent structure. You're expected to use AI tools from day one, before you've developed the judgment to evaluate what they produce. The calculator came after the multiplication tables. AI came before the fundamentals.

## The Evidence Is Worse Than the Theory

A [Harvard/BCG study](https://www.hbs.edu/faculty/Pages/item.aspx?num=64700) tested 758 consultants using GPT-4 on realistic tasks. The headline results were encouraging: AI users completed 12% more tasks, 25% faster, with 40% higher quality.

But when tasks fell outside AI's capability boundary, novices experienced a 19-percentage-point performance drop. They did worse with AI than without it. Experts succeeded by identifying flawed suggestions and rejecting them.

This is the jagged frontier problem. AI is brilliant at some things and confidently wrong about others. The boundary is invisible. You can't know where the frontier is without expertise to evaluate the output. AI is most useful to the people who need it least.

## But Maybe Fundamentals Are Changing

That's the case against giving kids AI too early. But I should steelman the other side.

The "fundamentals first" assumption might be historical rather than necessary. A generation fluent in AI tools from childhood might develop a different kind of judgment, one we can't conceptualize because we're anchored to our own experience. We worried calculators would erode mathematical intuition. Maybe it did. Maybe it didn't matter.

People lost the ability to navigate without GPS. This was fine until it wasn't. Dead zones, failures, moments when you actually need the skill you never built. A small cost for most, catastrophic for a few. The question is whether AI dependency follows the same pattern, or whether the stakes are higher.

And the creative reversal hypothesis (that [creativity becomes more valuable](/2025/09/02/the-human-edge/) as technical execution gets commoditized) has survivorship bias. Top-tier creative work requires depth and taste. But most creative work isn't top-tier. The middle of the creative market is being hollowed out. Stock photography is dying. Generic illustration is dying. Telling kids "be creative" might be accurate at the top of the distribution and catastrophic for the median.

I should also ask: who benefits from the "new entrants are at risk" narrative? Does it serve experienced workers protecting their position? Educators justifying traditional methods? I've thought about this. I don't think it serves anyone in particular. Everyone has a role to fulfill. The experienced workers who might benefit from this narrative are also the ones training the juniors, cleaning up the messes, and worrying about their own kids. There's no cartel here. Just genuine uncertainty.

The honest position is uncertainty.

## The Path Is the Same Regardless

So what do we do with that uncertainty? Here's where I've landed: I don't have an AGI prediction, and I don't think it matters.

Dario Amodei thinks AGI might arrive by 2027. Demis Hassabis gives it 50% by 2030. The gap matters less than what both agree on: this is happening faster than most realize, and the disruption will be severe. Whether it's three years or ten, the preparation path is the same.

Value will be derived by smart people at every step until we reach AGI or ASI or whatever comes next. This period is either the destination or the training ground. Either way, the work is the same: develop something worth multiplying.

The people I admire most are both highly creative and deep experts in their domain. The combination. That's always been true, and I don't see why AI changes it.

## What I'll Tell My Kids

So what will I actually say when my kids ask about careers?

I don't know. But here's what I'm doing now: I'm helping them develop creativity. Helping them think critically around problems. Encouraging them to find novel ways of sorting things out. Not the first solution. Not the obvious one. Their own.

I think these skills will survive. Creativity, critical thinking, the ability to see around corners. These seem durable. They're what AI multiplies when you have them, and what AI can't give you if you don't.

But maybe I'm wrong. Maybe the skills that seem timeless will turn out to be the next thing automated. Maybe the uncertainty is the point.

The paradox I opened with doesn't resolve neatly. I can't tell them to avoid AI, and I can't tell them to embrace it uncritically. So I'm preparing them for uncertainty itself. Teaching them to think, to question, to create. And hoping that's enough. It's the only honest answer I've got.
