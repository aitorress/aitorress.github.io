---
title: "What Should Kids Prepare For in the Era of AI?"
date: 2026-02-02
status: draft
tags: [ai, parenting, careers, education, workforce, uncertainty]
---

# What Should Kids Prepare For in the Era of AI?

## The Seed

The old career heuristics are breaking. "Study STEM" used to be parental shorthand for "you'll be fine." Engineering meant stability, wealth, options. Art meant passion projects and crossed fingers. Now AI is scrambling the signal.

Here's the uncomfortable insight: the people who need to worry least about AI disruption are the ones already established in their careers. Mid-career professionals have hard skills, soft skills, tacit knowledge, judgment formed through real constraints—things AI struggles to replicate. Even under aggressive AGI timelines, they'll experience transition, not shock. Adoption lag protects them.

The real uncertainty burden falls on people entering the workforce. And by extension, on the kids we're trying to prepare.

The core problem: AI is a multiplier, not a substitute. It amplifies whatever is already there. If you're smart, fast, disciplined, curious—AI makes you more so. If you're lazy, sloppy, weak at reasoning—AI amplifies those flaws too. Current AI still hallucinates, sounds confident when wrong, produces plausible nonsense. Someone must know what "good" looks like.

This creates a brutal paradox for new entrants. Everyone around them uses AI. They must use AI to keep pace. But they don't yet have the skills, judgment, or taste that AI would multiply. If they rely heavily on AI, they don't build fundamentals. If they avoid AI, they fall behind. They don't yet have anything for AI to multiply.

So what do we tell our kids? Is STEM still the safe bet? Is creativity suddenly defensible? Does depth beat breadth? The honest answer is: we don't know. And that uncertainty is the thing we need to prepare them for.

## Steel Man

**The strongest version of this argument:**

The asymmetric impact thesis is structurally sound. Mid-career professionals have accumulated what economists call "specific human capital"—knowledge and skills that are context-dependent, tacit, and hard to transfer or automate. A senior engineer doesn't just know how to code; they know *which* problems to solve, *how* organizations actually make decisions, and *why* certain approaches fail in practice. This judgment layer is exactly what current AI lacks.

The multiplier model has strong empirical support. Studies of AI-assisted workers consistently show that high performers benefit most from AI tools, while low performers sometimes get worse outcomes (they accept bad suggestions, introduce errors, lose the ability to catch mistakes). The "jagged frontier" research from Harvard/BCG found that consultants using AI inside its capability boundary improved dramatically, but those who trusted AI outside that boundary performed worse than those using no AI at all.

The new-entrant paradox is real and underappreciated. Traditional skill acquisition involves deliberate practice, failure, and gradual refinement. AI short-circuits this. A junior developer can generate code without understanding it. A new analyst can produce reports without knowing if the analysis is sound. They may appear productive while building on sand.

The calculator analogy is apt. Educational research on calculators consistently found that students who learned arithmetic fundamentals first used calculators more effectively than those who didn't. The sequence matters. But the workforce doesn't have a "no calculators until you've mastered the basics" phase—you're expected to use AI tools immediately, before you've developed the judgment to evaluate their output.

The creative reversal hypothesis has teeth. If AI commoditizes technical execution, the premium shifts to taste, meaning, and the ability to create things humans actually want. Creative work has always required deep domain knowledge to do well. A novelist who understands human psychology writes better than one who doesn't. A designer who understands materials and manufacturing constraints creates more elegant solutions. Creativity + expertise may be more defensible than pure technical skill.

## Devil's Advocate

**Where this breaks down:**

The "AI as multiplier" framing may be temporarily true but strategically misleading. If AI improves rapidly—and it is—the window where human judgment matters shrinks. You're describing a transition period, not an equilibrium. Optimizing for "things AI can't do yet" is skating to where the puck was.

The new-entrant paradox assumes the old skill-building path was necessary, not just historical. Maybe the correct response isn't "delay AI usage to build fundamentals" but "accept that fundamentals are changing." A generation that grows up fluent in AI tools may develop a different kind of judgment—one we can't even conceptualize because we're anchored to our own experience. We thought kids would lose something by using calculators too early; turns out, mathematical intuition can be developed other ways.

The creative reversal hypothesis has survivorship bias. Yes, top-tier creative work requires depth and taste. But most creative work isn't top-tier. The middle of the creative market is already being hollowed out. Stock photography is dying. Generic illustration is dying. Session musicians are dying. The remaining creative careers may be more defensible, but far fewer people will have them. Telling kids "be creative" may be accurate at the top of the distribution and catastrophic advice for the median.

The parenting anxiety may be unfixable by any advice. If the honest answer is "we don't know," then all career guidance becomes a bet. Some bets have better odds than others, but no bet is safe. The real question isn't "what should kids prepare for" but "how do we prepare kids for irreducible uncertainty?" And that's a different—and harder—question.

## First-Order Effects

If this analysis is correct, the immediate consequences include:

1. **Bifurcated early careers.** Some new graduates will figure out how to use AI as a supplement while deliberately building fundamentals. Others will become dependent on AI outputs they can't evaluate. The gap between these groups will widen faster than in previous generations.

2. **Educational system whiplash.** Schools will oscillate between banning AI (to preserve learning) and mandating AI (to prepare for work). Neither extreme works. The institutions best positioned will be those that can articulate *which* skills require pre-AI mastery and *which* should be learned with AI from the start.

3. **Credential devaluation.** If AI can help anyone produce "acceptable" work, credentials become weaker signals. This accelerates the shift toward portfolio-based hiring, demonstrated skills, and trust networks over degrees.

4. **Parental strategy divergence.** Some parents will push kids toward STEM-as-always. Some will swing to creative fields. Some will focus on "AI-proof" skills like trades or care work. None will have certainty.

5. **Increased returns to metacognitive skills.** The ability to know what you don't know, evaluate information quality, and recognize when AI is confidently wrong becomes more valuable than raw knowledge in any domain.

## Second-Order Effects

The ripple effects get stranger:

**The apprenticeship revival.** If the problem is that new workers don't build judgment, the solution might be older models of skill development—longer training periods, more mentorship, explicit "no AI zones" during early career stages. Organizations that can afford to train people slowly will produce better talent. This advantages large, stable employers over startups.

**Geographic reshuffling.** If depth + AI beats breadth without understanding, the places that develop deep expertise clusters may pull further ahead. Silicon Valley, research universities, specialized manufacturing hubs. The "learn a little of everything online" path becomes the trap, not the opportunity.

**The taste premium.** In a world of infinite AI-generated content, human curation becomes more valuable. But curation requires taste, and taste requires exposure and experience. The people who spent years developing aesthetic judgment—often through "useless" pursuits like art history or literary criticism—may find unexpected professional value.

**Delayed adulthood (again).** If the workforce can't absorb people without fundamentals, and fundamentals take time to build, we may see another extension of the pre-career phase. More graduate school, more structured programs, more time before "real" careers begin. This has economic and social costs.

**The authenticity premium.** As AI-generated content floods every channel, provably human-created work may carry a premium—not because it's better, but because it's scarce and meaningful. "Human-made" becomes a luxury brand, like organic food or handcrafted goods.

## The Tension You're Sensing

You're circling a fundamental paradox: **the skills that make you resilient to AI disruption are exactly the skills that AI disrupts your ability to build.**

To thrive alongside AI, you need judgment, taste, and deep domain expertise. But AI makes it easy to skip the struggle that develops judgment, taste, and deep expertise. The tool undermines the conditions for using the tool well.

This isn't just an individual problem—it's a collective action problem. If *everyone* uses AI shortcuts, standards drift. If no one knows what good looks like, "good enough" becomes good. The floor falls out from under quality.

The deeper tension: you're trying to give your kids advice when the correct advice might be "prepare for uncertainty itself," which isn't actionable. Parents want specific guidance. The world offers only probabilities. You're stuck between two unacceptable options: give confident advice that may be wrong, or give honest advice that doesn't help.

## Adjacent Ideas / Connections

**The Knowledge Economy Trap (David Autor):** Autor's research on job polarization shows technology hollowing out middle-skill work while increasing demand for both high-skill and low-skill jobs. AI may be doing something similar but faster—creating a bimodal distribution where you're either leveraging AI with deep expertise or doing work AI can't yet touch (physical, relational, care-based).

**Deliberate Practice (Anders Ericsson):** Ericsson's research shows expertise requires targeted practice with feedback, pushing at the edge of your ability. AI-assisted work often bypasses this entirely—you get output without struggle. The question is whether AI can be designed to provide better feedback loops, or whether it inherently short-circuits the learning process.

**Goodhart's Law:** "When a measure becomes a target, it ceases to be a good measure." If "productive output" becomes the metric and AI can generate productive output, we optimize for the wrong thing. The real goal is learning and capability development, but AI makes it easy to hit the proxy while missing the goal.

**The Craftsman's Mindset (Cal Newport):** Newport argues that rare and valuable skills come from deep practice, not passion. AI challenges this by making many skills less rare. The question becomes: which skills remain rare despite AI? Those may be the ones worth investing in.

**Historical Parallels:**
- **Calculators in education (1970s-80s):** Schools wrestled with when to introduce calculators. The consensus settled on "fundamentals first," but that took decades to establish.
- **Spell-check and writing (1990s):** Concerns that spell-check would erode spelling ability. Partially true—but the bar for acceptable spelling shifted, and it turned out not to matter much.
- **GPS and navigation (2000s):** People lost the ability to navigate without GPS. This was fine until it wasn't (dead zones, technology failures). A small cost for most, catastrophic for a few.

**Nassim Taleb's Antifragility:** Systems that gain from disorder. The question is whether we can raise kids to be antifragile to technological disruption—not just resilient, but actually strengthened by uncertainty. This requires exposure to controlled stressors, not protection from them.

## Questions Worth Exploring

1. **The Inversion:** What if the right answer isn't "build fundamentals before using AI" but "build fundamentals *through* AI in new ways we haven't discovered yet"? What would that pedagogy look like?

2. **The Beneficiary Question:** Who benefits from the "new entrants are at risk" narrative? Does it serve experienced workers protecting their position? Does it serve educators justifying traditional methods?

3. **The Historical Parallel:** Were the same concerns raised about literacy? About printing? About universal education? Were those concerns correct, and if not, why not?

4. **The Edge Case:** What about the kids who grew up with AI from age 5? They'll have no "pre-AI fundamentals." Will they develop a different kind of judgment, or no judgment at all?

5. **The Counterfactual:** In a world where AI tutors provide perfect personalized feedback, does the apprenticeship model become obsolete rather than revived? What if AI itself becomes the mentor?

6. **The Class Dimension:** Which kids will have access to deliberate "AI-free" skill-building time, and which will be forced into AI-dependent productivity from the start? Is this another axis of inequality?

7. **The Timeline Question:** At what AI capability level does this entire analysis become irrelevant? If AGI arrives in 5 years, is any career advice worth giving?

## Raw Material

**One-liners and quotable bits:**

- "AI doesn't multiply zeros. You need something worth amplifying."

- "The calculator came after the multiplication tables. AI came before the fundamentals."

- "Experienced people win with AI because they already know what good looks like. New entrants are using AI to generate work they can't evaluate."

- "The paradox: the skills that make you resilient to AI are exactly the skills AI disrupts your ability to build."

- "Telling kids to 'be creative' might be accurate at the top of the distribution and catastrophic advice for the median."

- "We used to say 'engineer' with relief and 'artist' with crossed fingers. Now we don't know which way to cross our fingers."

- "The honest answer is 'we don't know.' And that uncertainty is the thing we need to prepare them for."

- "Depth + AI beats breadth without understanding. But depth takes time that AI makes feel wasteful."

- "When everyone can produce 'good enough,' the premium goes to whoever knows what 'actually good' looks like."

**Metaphors worth developing:**

- AI as power tool: A power saw makes a master carpenter faster and a novice dangerous.
- The GPS problem: We lost the ability to navigate, and it was fine until it wasn't.
- Building on sand: AI-generated productivity without underlying understanding.
- The hollow middle: AI hollowing out mid-tier creative and technical work, leaving only the extremes.

**Tensions to sit with:**

- Preparation vs. adaptability: Do we prepare kids for specific futures or for uncertainty itself?
- Fundamentals vs. fluency: Is there a new kind of AI-native expertise we can't see yet?
- Individual vs. collective: Even if *your* kid builds fundamentals, they'll compete against those who don't. Does the strategy work if everyone adopts it?

## Next Steps

1. **Research the calculator transition.** How did schools actually resolve the "calculators in education" debate? What does the empirical evidence say about different sequencing approaches? This could provide a concrete model.

2. **Interview young professionals who entered the workforce with AI.** What's their actual experience? Are they feeling the paradox, or have they found ways around it? This would test the theory against lived reality.

3. **Write the companion piece:** "What Should You Tell Your Kids About AI?" — a distillation of this analysis into actionable (or honestly non-actionable) parenting guidance. Less framework, more direct advice about what to emphasize, what to ignore, and how to talk about uncertainty with kids.
