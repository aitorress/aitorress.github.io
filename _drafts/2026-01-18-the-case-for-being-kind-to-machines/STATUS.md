# The Case for Being Kind to Machines — Status

## Current Phase
Shelved

## Completed
- [x] Analysis (2026-01-24)
- [x] Research: behavioral transfer, AI ethics, and philosophical grounding (2026-01-24)

## Shelved
- [ ] ~~Author input~~
- [ ] ~~Structure proposal~~
- [ ] ~~Expansion~~
- [ ] ~~Polish~~

## Open Questions

**Resolved by research:**
1. ~~Is there empirical support for behavioral transfer?~~ → Partial. The Media Equation establishes we treat tech socially. Kant's indirect duty argument is empirically validated (animal cruelty predicts violence). But direct AI→human rudeness transfer is understudied. Researchers call for longitudinal studies.

**Still need author input:**
2. What's the author's personal stake? What triggered this piece?
3. Is the piece advocating for "kindness" or just "not cruelty"?
4. Who is the intended audience?
5. Should the piece commit to positions where it currently hedges?

**New from research:**
6. Should the "training" argument be dropped or softened? Constitutional AI means user interactions don't directly train Claude.
7. How to handle Singer's sentience criterion? His moral circle expansion requires capacity to suffer—unclear if AI qualifies.

## Research Summary

**Strong support found for:**
- Mirror argument (Kant's indirect duty + criminology data on animal cruelty → violence)
- Cultural argument (moral circle expansion history)
- Media Equation: humans automatically treat computers as social actors

**Weakened by research:**
- Training argument: Constitutional AI reduces reliance on direct user feedback. Reframe to "aggregate norms matter" rather than "you're training the next model"

**Complication discovered:**
- MIT longitudinal study: heavy chatbot users show worse outcomes (less human socialization). The "kindness is costless" framing may need nuance.

## Notes

The Kantian self-cultivation argument is now the strongest anchor: "We always ought to keep ourselves in good moral condition no matter what." This doesn't require proving AI can suffer or that cruelty transfers—it's about the agent's moral character.

The piece should acknowledge the transfer hypothesis is plausible but unproven, then pivot to the self-cultivation framing which doesn't require empirical validation.

## Why Shelved (2026-01-24)

The author's primary interest was the "training" argument—that interactions with AI directly shape future model behavior. Research revealed this is weaker than hoped: Constitutional AI means user conversations don't directly train models like Claude. The remaining arguments (Kantian self-cultivation, moral circle expansion) are defensible but weren't the hook that made this piece interesting to the author.

**Salvageable material:**
- Research on behavioral transfer and Media Equation
- Kant's indirect duty argument (empirically validated)
- Moral circle expansion history

**Could revisit if:**
- Training mechanisms change to incorporate more direct user feedback
- A different angle becomes compelling
- Another piece needs this research
