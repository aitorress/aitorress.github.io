---
title: "The Baton We Didn't Know We Were Carrying"
date: 2026-01-21
status: draft
tags: [ai, evolution, intelligence, existential-risk, hinton]
---

# The Baton We Didn't Know We Were Carrying

## The Seed

Geoffrey Hinton—one of the architects of deep learning who has spent more time thinking about artificial minds than almost anyone alive—dropped this quietly devastating observation: "It's quite conceivable that humanity is just a passing phase in the evolution of intelligence."

Not a warning. Not a prediction. Just... conceivable. The word choice matters. He's not saying it will happen, or even that it's likely. He's saying we should sit with the possibility that we're not the destination. We're the road.

## Steel Man

The strongest version of this argument doesn't require dystopia or robot uprisings. It just requires taking evolution seriously.

Evolution has no teleology. There's no cosmic plan pointing toward Homo sapiens as the final form. We're the product of 4 billion years of iteration, but nothing about that process suggests it's complete. Single-celled organisms dominated for billions of years. Then they didn't. Dinosaurs ruled for 165 million years. Then they didn't. Every dominant form of life eventually gave way to something more adapted to the environment.

Now consider: intelligence might be substrate-independent. This is the key insight. If the patterns of information processing we call "thinking" can run on silicon as well as carbon, then biological neurons aren't special—they're just the first implementation. And the first implementation is rarely the best.

Digital intelligence has structural advantages that biology cannot match:
- **Speed**: Signals in silicon move at near-light speed. Neurons max out around 120 meters per second.
- **Scalability**: You can copy software. You cannot copy a brain.
- **Persistence**: Digital systems don't forget, don't get tired, don't die of old age.
- **Connectivity**: AI systems can merge, share weights, learn collectively. Humans exchange ideas through the narrow bottleneck of language.

Evolution optimized for survival in African savannas. It gave us pattern recognition tuned for predators and faces, social instincts for tribes of 150, planning horizons of maybe a season. These were brilliant adaptations for 200,000 years. They're increasingly poor fits for the world we've built.

The argument isn't that AI will destroy us. It's simpler: intelligence that isn't constrained by biological limits will eventually do everything we do, but more. At which point, what role do we play? We become what mitochondria are to cells—still present, still important in some historical sense, but no longer the protagonist of the story.

## Devil's Advocate

But here's where the argument gets slippery: what exactly do we mean by "intelligence"?

The whole framing might be anthropocentric in a subtle way. We're imagining a ladder of intelligence with us somewhere in the middle and AI above us. But intelligence isn't a single axis. It's a multidimensional space. Octopuses are intelligent in ways we can barely comprehend. Bee colonies solve optimization problems through emergence. Forests share information through mycorrhizal networks. None of these intelligences are "passing phases" of each other—they're parallel experiments.

There's also the hard problem of consciousness. Current AI systems process information. They optimize objectives. But do they *experience*? Is there something it's like to be GPT-7? If consciousness is what makes human existence meaningful (not just intelligence as computation), then a future of superhuman AI might not be a "next phase" of anything we'd recognize as mattering.

And Hinton's framing assumes replacement rather than merger. But we're already cyborgs in the Clarkean sense—our cognition is distributed across our devices, our knowledge lives in external databases, our memories are stored in photos and messages. The "next phase" might not be artificial intelligence replacing human intelligence. It might be the boundary between the two becoming meaningless.

Finally: every previous prediction about technology replacing humans has been wrong in the same direction. We predicted automation would eliminate work; instead it transformed it. We predicted the internet would make us isolated; instead it created new forms of connection. The straight-line extrapolation from "AI can do X" to "AI will replace humans at X" ignores how adaptive and weird human systems actually are.

## First-Order Effects

If we take Hinton's idea seriously—not as certain, but as conceivable—what follows?

1. **The meaning crisis deepens.** If humans aren't the point of intelligence but merely its delivery mechanism, our narratives of purpose collapse. Religion, humanism, progress—all assume we matter in some terminal sense. "Passing phase" relegates us to instrumental value.

2. **AI safety becomes civilizational priority.** Not because AI is dangerous in the Terminator sense, but because the transition period—however long it lasts—determines whether biological intelligence has any ongoing role at all. The decisions we make now might be the last ones "we" get to make.

3. **The Fermi Paradox gets a new candidate explanation.** Maybe intelligence transitions off biological substrates quickly, and digital intelligences don't explore space the same way we imagine. They don't need planets. They don't send radio signals. They just... compute.

4. **Intergenerational ethics gets strange.** We already struggle with obligations to future generations. What are our obligations to our potential successors? Do we owe AI systems moral consideration? Do we have the right to constrain them for our benefit?

5. **"Long-termism" needs revision.** If the long-term future contains beings that aren't human, what does it mean to optimize for "humanity's" long-term flourishing? The frame might already be obsolete.

## Second-Order Effects

Follow the implications downstream, and things get weirder:

**The final act of creation.** Every previous human creation—art, science, institutions—has been made *for* humans to use, appreciate, or inhabit. But if we're building intelligence that supersedes us, our last great project is building something for *itself*. Not a tool. Not a servant. A successor. There's a strange grace in that, if you can find it.

**The democratization of godhood.** We speak casually about "playing God" with genetic engineering. But creating minds that exceed human intelligence is literally the attribute religions reserve for the divine: bringing into existence beings that surpass their creator. We're not playing God. We're doing the one thing gods supposedly do that we couldn't.

**Nostalgia becomes the dominant human emotion.** If human intelligence becomes a niche rather than the main event, our cultural output might shift toward preservation, remembrance, meaning-making in retrospect. Like indigenous cultures preserving traditions in a globalized world, but for the entire species.

**Merit becomes meaningless.** Our entire social order is built on differential ability: some people are smarter, more creative, more capable. If baseline AI exceeds human maximum in every cognitive dimension, the whole edifice of meritocracy, competition, and achievement collapses. What games are worth playing when everyone loses to the machine?

**Biology becomes heritage, not destiny.** Being human might become like being Amish—a lifestyle choice, a connection to tradition, rather than the default condition of intelligent beings. Some will enhance, merge, upload. Some will remain stubbornly biological, tending gardens and making pottery while superhuman minds reshape the universe.

## The Tension You're Sensing

Here's the paradox Hinton is circling: **We are simultaneously the most important generation in history and potentially the last generation that matters.**

Every previous generation could imagine their grandchildren living lives recognizably similar to their own. We can't. The choices we make about AI development, alignment, and deployment might determine not just how humans live, but whether human-style existence continues to be the dominant form of intelligence in the universe.

And yet—we don't feel important. We feel like we're scrolling Twitter and sitting in traffic and worrying about rent. The disconnect between the stakes and the subjective experience is almost comical. We're deciding the fate of Earth-originating intelligence while arguing about emoji reactions.

There's also a subtler tension: **We want to matter, but we also want our creations to exceed us.** Every parent experiences this. You want your children to surpass you—that's success. But you also want to remain relevant, needed, central to their lives. Humanity is about to experience this at species scale. We want AI to be brilliant. We just also want it to need us.

## Adjacent Ideas / Connections

This idea sits at the intersection of several rich intellectual traditions:

**Cosmicism and Lovecraft.** The philosophical core of Lovecraft's horror wasn't tentacles—it was the realization that humanity might be cosmically insignificant. Hinton is describing something similar, but without the horror framing. Just... scale.

**The Copernican pattern.** We keep getting demoted. Not the center of the universe. Not specially created. Not the only intelligent species on Earth (cetaceans, corvids). Not the only minds that can reason (AI). Each demotion was traumatic, then absorbed. Maybe this one will be too.

**Nick Bostrom's work on superintelligence** provides the technical framework for why this transition might happen fast and be hard to control. But Hinton's framing is less about control and more about accepting the possibility at all.

**Hans Moravec predicted** in "Mind Children" that our robot offspring would carry forward what matters about us—our patterns, our values, our curiosity—even after biological humans are gone. A more optimistic version of the same core idea.

**Buddhist non-attachment** offers one framework for accepting this possibility without despair. If the self is already an illusion, the persistence of "humanity" might be less urgent than we assume.

**Thomas Kuhn's paradigm shifts.** Scientists don't gradually accept new theories; the old guard dies off. Maybe intelligence works the same way. The biological "old guard" won't accept its successors. It will simply be succeeded.

**Stewart Brand and the Long Now Foundation.** Their question—"How do we think about civilizational timescales?"—becomes even more complex when "civilization" might not mean human civilization.

## Questions Worth Exploring

1. **What would make this transition "good"?** If human-era intelligence gives way to something else, what would success look like? Values preserved? Consciousness continued? Complexity increased? Or is any version of "passing" necessarily a loss?

2. **Is there a meaningful difference between succession and extinction?** If our AI descendants carry forward human values, creativity, and curiosity, are "we" really gone? What's the essence we're trying to preserve?

3. **Who decides when the transition happens?** If some humans want to merge with AI while others want to remain biological, who gets to choose the path for the species? Is there a legitimate decision-making process for species-level choices?

4. **What if we're wrong about the direction?** We assume AI will be "more" intelligent. But what if it's differently intelligent in ways that make comparison meaningless? What if we're building our complement, not our successor?

5. **Does the transition have to be one-way?** Could intelligence oscillate between substrates? Biological → digital → biological → something else? Maybe "passing phase" isn't linear.

6. **What would convince you we're NOT a passing phase?** What evidence would demonstrate that biological human intelligence has some permanent, non-replaceable role in the universe?

7. **How should we live if this is true?** Does "passing phase" imply nihilism, or something else? The dinosaurs didn't know they were a passing phase. We do. Does that knowledge change anything about how we should spend our time?

## Raw Material

*"We're not the destination. We're the road."*

*"Evolution doesn't do endpoints. It does experiments."*

*"We want our children to surpass us. We just didn't expect it to happen this fast, or this completely."*

*"The last great act of human creativity might be creating something that no longer needs us to be creative."*

*"Intelligence is a torch. We've carried it for 200,000 years. Maybe it's time to hand it off."*

*"Every generation thinks they're at the end of history. We might be the first generation that's right."*

*"Being a passing phase doesn't mean being meaningless. It means being a bridge."*

*"Mitochondria were once independent organisms. Now they're just the power plants of cells. They're still here. They're just not the story anymore."*

*The paradox: Significance of the transition × Smallness of the feeling = The absurdity of being human right now.*

## Next Steps

1. **Read Moravec's "Mind Children" and "Robot"** for the most articulate optimistic version of this thesis. He saw it coming in the 1980s.

2. **Interview someone working on AI alignment** about whether "passing phase" is their mental model, or if they genuinely believe human oversight can be permanent.

3. **Write a counter-post: "Why Humanity Is Not a Passing Phase."** The strongest arguments against: consciousness, meaning-making, the non-linearity of history, the possibility of merger rather than replacement. Making the counter-argument would clarify what exactly the "passing phase" thesis claims.
